# -*- coding: utf-8 -*-
"""zine_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQb7yBH6H52lHQL0R69fc6gc_3k8LkcC
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d mrwellsdavid/unsw-nb15

import zipfile
zip_ref = zipfile.ZipFile('/content/unsw-nb15.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

train_df=pd.read_csv('UNSW_NB15_training-set.csv')
test_df=pd.read_csv('UNSW_NB15_testing-set.csv')

train_df

test_df.loc[10:50]

# Combine datasets for preprocessing
df = pd.concat([train_df, test_df], axis=0)

df

df.info()

df['attack_cat'].value_counts()

df.isnull().sum()

df.shape

df['label'].value_counts()

X=df.drop(columns=['label','attack_cat'])
y = df['label']

cat_col= X.select_dtypes(include='object').columns
num_col= X.select_dtypes(exclude='object').columns

X = pd.get_dummies(X, columns=cat_col, drop_first=True)

# Scale numerical columns
scaler = StandardScaler()
X[num_col] = scaler.fit_transform(X[num_col])

x_train, x_test, y_train , y_test = train_test_split(X,y, test_size=0.2, random_state=45)

!pip install xgboost

from xgboost import XGBClassifier
model=XGBClassifier()

param_grid={
    'n_estimators': [50,100],
    'max_depth': [5,9],
    'learning_rate': [0.05, 0.1, 0.2,0.3]

}

# Step 4: Instantiate GridSearchCV
grid_search = GridSearchCV(estimator=model,
                           param_grid=param_grid,
                           scoring='accuracy',   # You can choose other scoring metrics
                           cv=3,                 # Number of cross-validation folds
                           verbose=1,
                           n_jobs=-1)            # Use all available cores

# Step 5: Fit the grid search to the training data
grid_search.fit(x_train, y_train)

# Step 6: Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validation Score:", best_score)

# Step 7: Evaluate the best model on the test set
best_model = grid_search.best_estimator_
predictions = best_model.predict(x_test)

print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions))
print("\nClassification Report:")
print(classification_report(y_test, predictions))

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, predictions), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for DoS and Worms')
plt.show()

